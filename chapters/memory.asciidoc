[[CH-Memory]]
== The Memory Subsystem: Stacks, Heaps and Allocators

Before we dive into the memory subsystem of ERTS, we need to have some
basic vocabulary and understanding of the general memory layout of a
program in a modern operating system. In this review section I will
assume the program is compiled to an ELF executable and running on
Linux on something like an IA-32/AMD64 architecture. The layout and
terminology is basically the same for all operating systems that ERTS
compile on.

A program's memory layout looks something like this:

[[program_memory_layout]]
.Program Memory Layout
[ditaa]
----
 high
 addresses
        +--------------+
        |   Arguments  |
        |     ENV      |
        +--------------+
        |    Stack     | --+
        |      |       |   | Can grow
        |      v       |   | dynamically
        |              | --+
        +--------------+
        |              | -------------------------------+
        +--------------+                                |
        |    memory    |                                |
        |      map     | -- files or anonymous          |
        |    segment   |                                |
        +--------------+                                |
        |              |                                | Memory
        +--------------+                                | Mapping
        | Thread Stack | --+                            | Region
        |      |       |   | Statically allocated       |
        |      v       |   | on thread start.           |
        |              | --+                            |
        +--------------+                                |
        |              |                                |
        +--------------+                                |
        | Thread Stack | --+                            |
        |      |       |   | Statically allocated       |
        |      v       |   | on thread start.           |
        |              | --+                            |
        +--------------+                                |
        |              | -------------------------------+
        +--------------+ brk
        |              | --+
        |      ^       |   | Can grow
        |      |       |   | dynamically
        |     Heap     | --+
        +--------------+ start_brk
        |     BSS      | --  Static variables initialized to zero
        +--------------+
        |     Data     | --+
        +--------------+   | Binary (disk image)
        |     Code     | --+
        +--------------+
 low
 addresses


----


Even though this picture might look daunting it is still a
simplification. (For a full understanding of the memory subsystem read
a book like "Understanding the Linux Kernel" or "Linux System
Programming") What I want you to take away from this is that there are
two types of dynamically allocatable memory: the heap and memory
mapped segments. I will try to call this heap the _C-heap_ from now
on, to distinguish it from an Erlang process heap. I will call a
memory mapped segment for just a _segment_, and any of the stacks in
this picture for the _C-stack_.

The C-heap is allocated through malloc and a segment is allocated with
mmap.

=== The memory subsystem

Now that we dive into the memory subsystem it will once again
be apparent that ERTS is more like an operating system than just a
programming language environment. Not only does ERTS provide a garbage
collector for Erlang terms on the Erlang process level, but it also
provides a plethora of low level memory allocators and memory
allocation strategies.

For an overview of memory allocators see the erts_alloc documentation
at: http://www.erlang.org/doc/man/erts_alloc.html

All these allocators also comes with a number of parameters that
can be used to tweak their behavior, and this is probably one
of the most important areas from an operational point of view.
This is where we can configure the system behavior to fit anything
from a small embedded control system (like a Raspberry Pi) to an
Internet scale 2TB database server.

There are currently eleven different allocators, six different
allocation strategies, and more than 18 other different settings,
some of which are taking arbitrary numerical values. This
means that there basically is an infinite number of possible
configurations. (OK, strictly speaking it is not infinite, since
each number is bounded, but there are more configurations
than you can shake a stick at.)

In order to be able to use these settings in any meaningful way
we will have to understand how these allocators work and
how each setting impacts the performance of the allocator.

The erts_alloc manual goes as far as to give the following warning:

[quote, Ericsson AB, http://www.erlang.org/doc/man/erts_alloc.html]
____
WARNING: Only use these flags if you are absolutely sure what you are
doing. Unsuitable settings may cause serious performance degradation
and even a system crash at any time during operation.
____

Making you absolutely sure that you know what you are doing, that is
what this chapter is about.

Oh yes, we will also go into details of how the garbage collector
works.


[[SS-Memory_Allocators]]
=== Different type of memory allocators
The Erlang run-time system is trying its best to handle memory
in all situations and under all types of loads, but there are
always corner cases. In this chapter we will look at the details
of how memory is allocated and how the different allocators work.
With this knoweledge and some tools that we will look at later
you should be able to detect and fix problems if your system
ends up in one of these corner cases.

For a nice story about the troubles the system might get into and how to analyze
and correct the behavior read Fred Hébert’s essay
https://blog.heroku.com/archives/2013/11/7/logplex-down-the-rabbit-hole["Troubleshooting
Down the Logplex Rabbit Hole"].


When we are talking about a memory allocator in this book we
have a specific meaning in mind. Each memory allocator manage
allocations and deallocations of memory of a certain type.
Each allocator is intended for a specific type of data and is
often specialized for one size of data.

Each memory allocator implements the allocator interface that
can use different algorithms and settings for the actual
memory allocation.

The goal with having different allocators is to reduce
fragmentation, by grouping allocations of the same size,
and to increase performance, by making frequent allocations
cheap.

There are two special, fundamental or generic, memory allocator types
_sys_alloc_ and _mseg_alloc_, and nine specific allocators implemented
through the _alloc_util_ framework.

In the following sections we will go though the different allocators,
with a little detour into the general framework for allocators
(alloc_util).

Each allocator has several names used in the documentation and in the
C code. See xref:table-allocators[] for a short list of all allocators
and their names. The C-name is used in the C-code to refer to the
allocator. The Type-name is used in erl_alloc.types to bind allocation
types to an allocator. The Flag is the letter used for setting
parameters of that allocator when starting Erlang.


.List of memory allocators.
[[table-allocators]]
[options="header"]
|===============================================================================
|Name                    | Description           | C-name       | Type-name | Flag
| Basic allocator        | malloc interface      | sys_alloc    | SYSTEM    | Y
|Memory segment allocator| mmap interface        | mseg_alloc   | -         | M
| Temporary allocator    | Temporary allocations | temp_alloc   | TEMPORARY | T
| Heap allocator         | Erlang heap data      | eheap_alloc  | EHEAP     | H
| Binary allocator       | Binary data           |binary_alloc  | BINARY    | B
| ETS allocator          | ETS data              | ets_alloc    | ETS       | E
| Driver allocator       | Driver data           |driver_alloc  | DRIVER    | R
| Short lived allocator  | Short lived memory    | sl_alloc     |SHORT_LIVED| S
| Long lived allocator   | Long lived memory     | ll_alloc     |LONG_LIVED | L
| Fixed allocator        | Fixed size data       | fix_alloc    |FIXED_SIZE | F
| Standard allocator     | For most other data   | std_alloc    | STANDARD  | D
| Literal allocator      | Module constants      | literal_alloc| LITERAL   | *(none)*
|===============================================================================



==== The basic allocator: sys_alloc

The allocator sys_alloc can not be disabled, and is basically a
straight mapping to the underlying OS malloc implementation in
libc.

If a specific allocator is disabled then sys_alloc is used instead.

All specific allocators uses either sys_alloc or mseg_alloc to
allocate memory from the operating system as needed.

When memory is allocated from the OS sys_alloc can add (pad) a fixed
number of kilobytes to the requested number. This can reduce the
number of system calls by over allocating memory. The default padding
is zero.

When memory is freed, sys_alloc will keep some free memory allocated
in the process. The size of this free memory is called the trim
threshold, and the default is 128 kilobytes. This also reduces the
number of system calls at the cost of a higher memory footprint.
This means that if you are running the system with the default
settings you can experience that the Beam process does not give
memory back to the OS directly as memory is freed up.

Memory areas allocated by sys_alloc are stored in the C-heap of the
beam process which will grow as needed through system calls to brk.

==== The memory segment allocator: mseg_alloc

If the underlying operating system supports mmap a specific memory
allocator can use mseg_alloc instead of sys_alloc to allocate
memory from the operating system.

Memory areas allocated through mseg_alloc are called segments. When a
segment is freed it is not immediately returned to the OS, instead it
is kept in a segment cache.

When a new segment is allocated a cached segment is reused if
possible, i.e. if it is the same size or larger than the requested
size but not too large. The value of _absolute max cache bad fit_
determines the number of kilobytes of extra size which is considered
not too large. The default is 4096 kilobytes.

In order not to reuse a 4096 kilobyte segment for really small
allocations there is also a _relative_max_cache_bad_fit_ value which
states that a cached segment may not be used if it is more than
that many percent larger. The default value is 20 percent. That
is a 12 KB segment may be used when asked for a 10 KB segment.

The number of entries in the cache defaults to 10 but can be
set to any value from zero to thirty.

==== The memory allocator framework: alloc_util

Building on top of the two generic allocators (sys_alloc and mseg_alloc)
is a framework called _alloc_util_ which is used to implement specific
memory allocators for different types of usage and data.

The framework is implemented in _erl_alloc_util.[ch]_ and the different
allocators used by ERTS are defined in erl_alloc.types in
the directory "erts/emulator/beam/".

In a SMP system there is usually one allocator of each type per
scheduler thread.

The smallest unit of memory that an allocator can work with is called a
_block_. When you call an allocator to allocate a certain amount of
memory what you get back is a block. It is also blocks that you give
as an argument to the allocator when you want to deallocate memory.

The allocator does not allocate blocks from the operating system
directly though. Instead the allocator allocates a _carrier_ from the
operating system, either through sys_alloc or through mseg_alloc,
which in turn uses malloc or mmap. If sys_alloc is used the carrier
is placed on the C-heap and if mseg_alloc is used the carrier
is placed in a segment.

Small blocks are placed in a multiblock carrier. A multiblock carrier
can as the name suggests contain many blocks. Larger blocks are placed
in a singleblock carrier, which as the name implies on contains one
block.

What's considered a small and a large block is determined by the
parameter _singleblock carrier threshold_ (`sbct`), see the list
of system flags below.

Most allocators also have one "main multiblock carrier" which is never
deallocated.

[ditaa]
----
 high
 addresses
           |FREE OS MEMORY |
           +---------------+ brk
           |   FREE HEAP   |       | less than MYtt kb
           +---------------+
     /     |  Unused PAD   |  | multiple of Muycs
    |      |---------------|  |
    S      |               |  |    |
singleblock|               |  |    |
 carrier 1 |     Block     |  |    | larger than MSsbct kb
    |      |               |  |    |
     \     |               |  |    |
           +---------------+
     /     |Free in Carrier|       |
    |      |---------------|       |
    S      |               |       |
  main     |               |       |
multiblock |     Block 2   |       | MSmmbcs kb
 carrier   |---------------|       |
    |      |               |       |
     \     |     Block 1   |       |
           +---------------+
           |               |
           |    U S E D    |
           |               |
           +---------------+ start_brk
               C-Heap
 low
 addresses


----


===== Blocks, Carriers, and Allocation Strategies

When an Erlang process needs memory, it doesn't directly request it from the
operating system with each allocation. Instead, it interacts with specialized
allocators provided by the _alloc_util_ framework (implemented in
`erl_alloc_util.[ch]`). These allocators handle requests by distributing memory
from larger contiguous regions known as "carriers."

Carriers are memory regions allocated directly from the operating system. A
carrier is allocated either through:

 - `sys_alloc` (using standard C library functions like `malloc()`), placing
 memory on the process heap, or
 
 - `mseg_alloc` (using `mmap()`), placing memory outside the typical C-heap
 area.

Carriers are subdivided into smaller memory segments called "blocks." When
memory is requested, blocks are allocated from these carriers. There are two
primary carrier types:

 - **Multiblock Carrier**: These hold multiple smaller blocks, suitable for
 frequent, smaller allocations. By default, allocators typically request
 multiblock carriers in approximately 8 MB chunks (tunable via system flags),
 efficiently handling common Erlang memory patterns. 
 
 - **Singleblock Carrier**: Dedicated to exactly one large block. Allocations
 exceeding the single block carrier threshold are placed in
 single-block carriers directly allocated from the OS.

===== Carrier Threshold (`sbct`) and its Impact

The parameter known as _singleblock carrier threshold_ (`sbct`) determines the
size boundary between what's considered a "small" and "large" allocation.
Allocations larger than the `sbct` value use singleblock carriers, while smaller
allocations use multiblock carriers.

By default, the sbct threshold is set so that larger allocations, those
significantly bigger than typical Erlang terms—are isolated in single-block
carriers. Smaller objects, which constitute most allocations, efficiently share
space within multiblock carriers.

This threshold (sbct) is tunable if your application demonstrates unusual
allocation patterns. Adjusting it affects how the runtime system balances
between fragmentation (caused by large allocations in multiblock carriers) and
overhead (due to many single-block carriers).

===== Why Most Allocations are Preferably Small (Multiblock Carriers)

Multiblock carriers are favored for their efficiency with typical Erlang
workloads—many small, short-lived allocations. Research by the HiPE team has
shown most Erlang terms are small (less than eight words), fitting neatly into
these multiblock carriers.

A typical ERTS allocator usually maintains:

 - **One main multiblock carrier** per allocator to handle frequent, small-sized
 allocations. It rarely releases this carrier, reusing freed blocks internally
 to reduce fragmentation. 
 - **Multiple singleblock carriers**, each allocated
 individually for large objects or binaries. Once these blocks are deallocated,
 the singleblock carriers are returned to the OS immediately.

====== Memory Layout: Carriers and Blocks (Visualized)

To clarify visually:

[ditaa]
-----------------------------------------

hend ->  +----+
         |....|
stop ->  |    |
         |    |    +----+ old_hend
         |    |    |    |
htop ->  |    |    |    |
         |....|    |    | old_htop
         |....|    |....|
heap ->  +----+    +----+ old_heap
        The Heap   Old Heap

<High Memory Addresses>

+-------------------------------+  OS allocated
|  Singleblock Carrier (> sbct) |
+-------------------------------+
|                               |  
|    Large Allocation (1 block) |
|                               |
|-------------------------------|
|       Unused or padding       |
|-------------------------------|
|          Multiblock           |
|       Carrier (main)          |
|     Block 1 | Block 2 | ...   |
+-------------------------------+
|                               |
|        Allocated Heap         |
|-------------------------------| <- start_brk (OS allocation boundary)
|          C-Heap               |
+-------------------------------+
(low addresses)

-----------------------------------------


===== When to Adjust the `sbct`

Optimizing the singleblock carrier threshold (`sbct`) parameter is a matter of
understanding your application's memory allocation patterns. Increasing `sbct`
directs more allocations into multiblock carriers, improving memory reuse and
reducing fragmentation. This is especially beneficial if your application
frequently allocates moderate-sized data structures, causing fragmentation or
frequent OS-level memory requests.

Reducing `sbct`, however, forces more allocations into singleblock carriers,
making sense when your application occasionally allocates large memory blocks.
Managing these large allocations separately simplifies their reclamation and
prevents interference with smaller allocations.

If adjusting `sbct` alone does not resolve frequent minor garbage
collections—often due to numerous short-lived allocations—consider increasing
the process’s initial heap size (`min_heap_size`) to reduce allocation churn.

In practice, the default `sbct` setting is suitable for most Erlang
applications. Only fine-tune this parameter if profiling indicates specific
problems with fragmentation, memory overhead, or unusual allocation patterns.

===== Memory allocation strategies

To find a free block of memory in a multi block carrier an
allocation strategy is used. Each type of allocator has
a default allocation strategy, but you can also set the
allocation strategy with the `as` flag.

The Erlang Run-Time System Application Reference Manual lists
the following allocation strategies:

[quote,'http://www.erlang.org/doc/man/erts_alloc.html[erts_alloc]']
__________________________

_Best fit_: Find the smallest block that satisfies the requested block size.
(bf)

_Address order best fit_: Find the smallest block that satisfies the
requested block size. If multiple blocks are found, choose the one
with the lowest address.
(aobf)

_Address order first fit_: Find the block with the lowest address that
satisfies the requested block size.
(aoff)

_Address order first fit carrier best fit_ : 
Find the carrier with the lowest address that can satisfy the
requested block size, then find a block within that carrier using the
"best fit" strategy.  (aoffcbf)

_Address order first fit carrier address order best fit_: Find the
carrier with the lowest address that can satisfy the requested block
size, then find a block within that carrier using the "address order
best fit" strategy.
 aoffcaobf (address order first fit carrier address order best fit)


_Good fit_: Try to find the best fit, but settle for the best fit found
during a limited search.
(gf)

_A fit_: Do not search for a fit, inspect only one free block to see if
it satisfies the request. This strategy is only intended to be used
for temporary allocations.
(af)

__________________________


The choice of allocation strategy influences how efficiently free memory is
reused and directly impacts fragmentation and performance. The default strategy
for most allocators is often **best fit** (`bf`) or **address order best fit**
(`aobf`), which balances memory utilization against allocation speed.

Alternative strategies, such as **address order first fit** or **good fit**, can
be configured per allocator (using the `+M<S>as <strategy>` system flag).
Selecting a different strategy can mitigate memory fragmentation at the expense
of higher CPU overhead during allocation searches.


==== The temporary allocator: temp_alloc

The allocator _temp_alloc_, is used for temporary
allocations. That is very short lived allocations. Memory allocated
by temp_alloc may not be allocated over a Erlang process context
switch.

You can use temp_alloc as a small scratch or working area while doing
some work within a function. Look at it as an extension of the C-stack
and free it in the same way. That is, to be on the safe side, free
memory allocated by temp_alloc before returning from the function that
did the allocation. There is a note in erl_alloc.types saying that
you should free a temp_alloc block before the emulator starts
executing Erlang code.

Note that no Erlang process running on the same scheduler as the
allocator may start executing Erlang code before the block is freed.
This means that you can not use a temporary allocation over a BIF
or NIF trap (yield).

In a default R16 SMP system there is N+1 temp_alloc allocators where N
is the number of schedulers. The temp_alloc uses the "A fit" (`af`)
strategy. Since the allocation pattern of the temp_alloc basically is
that of a stack (mostly of size 0 or 1), this strategy works fine.

The temporary allocator is, in R16, used by the following types of
data: TMP_HEAP, MSG_ROOTS, ROOTSET, LOADER_TEMP, NC_TMP, TMP,
DCTRL_BUF, TMP_DIST_BUF, ESTACK, DB_TMP, DB_MC_STK, DB_MS_CMPL_HEAP,
LOGGER_DSBUF, TMP_DSBUF, DDLL_TMP_BUF, TEMP_TERM, SYS_READ_BUF,
ENVIRONMENT, CON_VPRINT_BUF.

For an up to date list of allocation types allocated with each
allocator, see erl_alloc.types
(e.g. `+grep TEMPORARY erts/emulator/beam/erl_alloc.types+`).

I will not go through each of these different types, but in
general as you can guess by their names, they are temporary
buffers or work stacks.


==== The heap allocator: eheap_alloc

The heap allocator manages memory blocks for a process’s private data: Erlang
terms stored on the process heap (new and old generations), plus related
structures like heap fragments and beam registers. Almost every Erlang term a
process creates ends up in memory obtained through `eheap_alloc`.

By default, each scheduler has one `eheap_alloc` instance, so memory for processes
running on that scheduler remains mostly local, reducing contention. The
allocator’s frequent tasks include:

* Process Heap: Each Erlang process has its own heap where it stores data such as
tuples, lists, maps, integers, and any small binaries (<= 64 bytes).

* Heap Fragments: If a process briefly needs more memory but cannot immediately GC
(e.g., constructing a large message), the VM may allocate a “heap fragment” from
eheap_alloc. On the next garbage collection, these fragments are merged or
freed. 

* Register Arrays: Some runtime-implementation details (e.g., the
“beam_registers” data structure) also use eheap_alloc. As an Erlang developer,
you typically optimize around eheap_alloc usage by controlling process heap
sizes or by understanding when your processes generate large, short-lived data.
Good practice includes carefully sizing process heaps if they frequently handle
big data, rather than allowing many minor GCs.


==== The binary allocator: binary_alloc

The binary allocator handles memory for (yes, you guessed it) binaries.
Specifically, it manages binaries larger than 64 bytes (called refc binaries),
storing them off-heap and using reference counting to track their usage. Each
process heap holds just a small wrapper (a `ProcBin`) pointing to the actual
binary.

These binaries vary widely, from modestly sized binaries (hundreds of bytes) up
to massive binaries measured in megabytes (such as entire file contents or
external data from networks).

The allocator has a few interesting characteristics:

* **Best-Fit Strategy:**  
  It chooses the smallest suitable free block to store a new binary, which
  reduces fragmentation, think of it as a meticulous Tetris player fitting
  binaries neatly into memory.

* **Reference Counting:**  
  Off-heap binaries stick around until the very last process stops referencing
  them. If just one forgetful process clings to a huge binary, it stays alive,
  sometimes annoyingly longer than you'd prefer.

* **Sub-Binaries (Slices):**  
  When you match binaries like `\<<X:32, Rest/binary>>`, the smaller binary
  (`Rest`) still references the original larger binary, avoiding copying is
  great, until you realize you've accidentally kept a giant binary alive just to
  reference a tiny bit. In these cases, calling `binary:copy/1` is your friend.

If memory mysteriously grows, check for large binaries hanging around due to
references lingering in processes. Occasionally, you might need to gently remind
the garbage collector (via `erlang:garbage_collect/1`) or let processes
hibernate to reclaim the memory faster.

==== The ETS allocator: ets_alloc

The allocator `ets_alloc` manages memory for your beloved ETS (Erlang Term
Storage) tables. By default, ETS tables aren't tied to individual processes,
meaning they hang around even when your processes take their garbage out. Keep
in mind a few key details:

* **Long-Lived Data:**  
  Once you place data into an ETS table, it moves out of process heaps and
  settles into its own cozy allocator. Standard process garbage collection won't
  tidy up this space. If you want memory back, you must explicitly remove data
  or delete the table.

* **Diverse Use Cases:**  
  This allocator deals with everything ETS-related—from classic hash tables to
  fancy `ordered_set` structures and internal metadata. Popular or large tables
  can easily become memory-hungry beasts, so watch out.

* **Short-Lived ETS Data:**  
  Sometimes ETS might borrow other allocators temporarily, for quick tasks like
  matching or intermediate results, but the main data lives in `ets_alloc`.

Because ETS tables can grow surprisingly large, occasionally check their
size with functions like `ets:info(Tab, memory)` or keep an eye on overall
`ets_alloc` usage through your system metrics.

==== The driver allocator: driver_alloc

The driver allocator handles memory for ports, linked-in drivers, and NIF
resources. In simpler terms, if you're reaching beyond Erlang's safety
net—interacting with I/O drivers, external libraries, or file descriptors—you'll
end up here. Key points to keep in mind:

* **Port & Driver Data:**  
  Allocations here include structures for network sockets, open file
  descriptors, and buffers specific to your linked-in drivers.

* **NIF-Allocated Data:**  
  When a NIF reaches out with `enif_alloc`, the memory ultimately comes from
  this allocator. The VM politely waits until you call `enif_free`, or your NIF
  object gracefully exits the stage, to reclaim this space.

* **Potential For External Leaks:**  
  Because NIFs and drivers bypass Erlang’s usual memory-safety rules, a
  misbehaving driver might unintentionally hold onto memory, creating the
  digital equivalent of a leaky faucet. Keeping an eye on your `driver_alloc`
  usage helps catch these drips.

Though developers rarely manipulate `driver_alloc` directly, it's wise to
monitor this allocator in production environments, especially if your 
are using NIFs.

==== The short lived allocator: sl_alloc

The short-lived allocator (`sl_alloc`) handles memory for data structures with
lifespans that surpass the blink-and-you-miss-it nature of temporary
allocations, but not by much. Think of it as memory that's sticking around just
long enough to say hello and have a quick coffee before heading out. Typical
examples include:

* **Intermediate Buffers:**  
  Small buffers needed for short operations that linger briefly across
  scheduling points—but definitely don't plan on staying overnight.

* **Ephemeral Lists:**  
  Temporary runtime structures—such as quick-fire system message buffers or
  short-lived scheduling metadata—that disappear almost as soon as you notice
  them.

Erlang/OTP leverages `sl_alloc` for transient operations like match state
objects, ephemeral I/O buffers, and other fleeting entities. Although these
allocations can outlive truly temporary memory, they're still expected to depart
swiftly, leaving minimal footprints behind.

If your application is particularly chatty—generating numerous short-lived
allocations (such as frequent small driver calls or brief
computations) `sl_alloc` might heat up significantly. Checking usage with tools
like `recon_alloc:usage()` can tell you whether `sl_alloc` is overused.

==== The long lived allocator: ll_alloc

The long lived allocator handles data intended to stay alive for extended
periods, often as long as the Erlang node itself. Typical examples include:

- **Atoms:** Once created, atoms persist indefinitely, making them permanent
residents of `ll_alloc`.

- **Loaded Modules and Code:** Compiled modules, exported functions, and
metadata related to anonymous functions (`funs`) are stored here. For instance,
when you load a module via `code:load_file(my_module)`, its metadata lands
in `ll_alloc`.

- **Scheduler and System Structures:** Internal runtime structures—like
scheduler run queues (`run_queue`), pollset information (`pollset`), and the
process registry (`proc_tab`)—live here because they're fundamental to VM
operations.

Because objects in `ll_alloc` tend to stick around, this allocator typically
grows slowly but continuously during a node's lifetime. If your system
frequently loads and unloads modules, you might see fluctuations. For instance,
repeatedly executing hot-code loading without properly unloading old versions
can gradually inflate memory usage in `ll_alloc`.

In practice, most systems won't reclaim much memory from here unless you're
explicitly unloading modules or performing node restarts. Hence, it's wise to
occasionally glance at `recon_alloc:usage(ll_alloc)` to detect unexpected spikes
or fragmentation—especially if your application dynamically manages modules or
extensive long-term data.

==== The fixed size allocator: fix_alloc

The fixed allocator, `fix_alloc`, specializes in allocating fixed-size
objects—typically small C structs whose size never changes (e.g., message
references, driver event data, monitors). Since these objects come in uniform
sizes, the allocator can efficiently handle them:

By default, fix_alloc uses "Address Order Best Fit",
returning freed objects neatly into lists of same-sized blocks, effectively
minimizing fragmentation. Think of it like perfectly stacking identical LEGO
bricks rather than randomly tossing different-sized pieces into a box.

Examples include internal VM structures like ErlMessage, monitor references, and
scheduler bookkeeping data.

While developers don't usually interact directly with fix_alloc, it's essential
at the system level. Efficient allocation here ensures the runtime isn't bogged
down managing tiny, fragmented allocations, helping Erlang keep its reputation
for handling concurrency gracefully.


==== The standard allocator: std_alloc

When memory doesn't neatly fit into Erlang's specialized allocator buckets, it
finds a home in `std_alloc`—the runtime's versatile catch-all allocator. Think
of it as Erlang’s “miscellaneous drawer” for memory.

This allocator handles a diverse mix of allocations. This can include references
to ephemeral data that's ambiguously short- or long-lived, dynamically sized
structures, VM subsystem data without clear categorization, or simply memory
allocations too unique to neatly classify elsewhere.

Just like the other allocators, you can adjust `std_alloc` behavior with startup
flags such as `+Ms` or `+Msbct`. Usually, you leave it alone, but it's good to
know it's tweakable when needed.

In troubleshooting, `std_alloc` can become a prime suspect for unexplained
memory spikes. Tools like `erlang:system_info({allocator, std_alloc})` or
`recon_alloc` can quickly reveal if it’s hoarding more memory than you
anticipated.

Since `std_alloc` gathers memory requests that defy neat categorization, it's
normal for it to accumulate significant usage on busy nodes. If you see this
allocator consistently growing, it's usually a sign to double-check application
behavior or revisit your assumptions about what's "normal."

==== The literal allocator: literal_alloc

The `literal_alloc` stores compile-time constants, often called the _literal
pool_, in loaded Erlang modules. Think of it as the VM's "read-only memory
shelf," where Erlang safely stores constants like large static binaries, tuples,
or lists defined at compile time.

Unlike typical allocators, `literal_alloc` is managed globally rather than per
scheduler, as literals aren't frequently modified or reclaimed during runtime.
Once loaded, these literals remain until their corresponding module is
explicitly purged or reloaded.

Monitoring `literal_alloc` is usually uneventful, but frequent dynamic module
updates or loading large literal-heavy modules repeatedly might make it worth a
glance.


=== Per-Scheduler Allocator Instances

In modern OTP (since R13B), most memory allocators described above (e.g.,
`eheap_alloc`, `binary_alloc`, `ets_alloc`) have multiple instances to minimize
contention in SMP (Symmetric Multi-Processing) environments. By default, each
scheduler thread maintains its own instance of these allocators, plus one
additional instance shared by driver async threads.

This per-scheduler allocation approach partitions memory management,
significantly reducing lock contention and improving scalability on multicore
systems. It also implies that memory usage and fragmentation are handled
separately for each scheduler, which can affect how you interpret memory
statistics or troubleshoot memory-related issues.

When examining allocator metrics (e.g., through `recon_alloc` or
`erlang:system_info`), you'll see statistics aggregated across multiple
allocator instances. Be aware of this when analyzing memory patterns.


=== System Flags for Memory

Memory allocator system flags follow this syntax:

```
+M<S><P> <V>
```

- `<S>` is a single uppercase letter identifying the allocator.
- `<P>` specifies the parameter.
- `<V>` specifies the value to use.

**Allocator identifiers (`<S>`):**

- `B`: binary_alloc  
- `D`: std_alloc  
- `E`: ets_alloc  
- `F`: fix_alloc  
- `H`: eheap_alloc  
- `I`: literal_alloc  
- `L`: ll_alloc  
- `M`: mseg_alloc  
- `R`: driver_alloc  
- `S`: sl_alloc  
- `T`: temp_alloc  
- `Y`: sys_alloc  
- `u`: alloc_util (affects all alloc_util-based allocators)

=== Commonly Used Flags

**Allocation Strategy (`as`):**

Determines how memory blocks are selected within carriers.

- `bf`: Best fit
- `aobf`: Address order best fit
- `aoff`: Address order first fit
- `aoffcbf`: Address order first fit carrier best fit
- `ageffcbf`: Age order first fit carrier best fit
- `gf`: Good fit
- `af`: A fit

Example:
```
+MBas bf
```
_(Binary allocator uses best-fit strategy.)_

**Singleblock Carrier Threshold (`sbct`):**  
Defines the threshold in KB above which allocations use singleblock carriers.

Example:
```
+MBsbct 1024
```
_(Binary allocations above 1024 KB use singleblock carriers.)_

**Multiblock Carrier Settings:**

- **Smallest (`smbcs`) and Largest (`lmbcs`) Multiblock Carrier Size:**  
  Control minimum/maximum sizes for multiblock carriers (KB).
- **Carrier Growth Stages (`mbcgs`):**  
  Defines carrier size growth between minimum and maximum.

Example:
```
+MBsmbcs 512 +MBlmbcs 8192
```
_(Binary allocator multiblock carriers range from 512 KB to 8 MB.)_

**Abandon Carrier Utilization Limit (`acul`):**  
Percentage threshold below which carriers are abandoned and reused.

Example:
```
+MBacul 50
```
_(Binary carriers with utilization below 50% are marked abandoned.)_

**Abandon Carrier Free Utilization Limit (`acful`):**  
Below this utilization, the VM informs the OS that unused memory can be reclaimed.

Example:
```
+MDacful 10
```
_(std_alloc marks memory as reclaimable by OS if utilization is below 10%.)_

**Multiple Thread-specific Instances (`t`):**  
Controls if allocators use multiple instances (one per scheduler).

Example:
```
+MHt true
```
_(eheap_alloc uses separate allocator instances per scheduler thread.)_

**Allocation Tagging (`atags`):**  
Adds tags to allocations, useful for debugging with instrumentation.

Example:
```
+MRatags true
```
_(Enable tagging for driver_alloc.)_

=== Special Flags for `mseg_alloc`

`mseg_alloc` (Memory Segment Allocator) has specific settings:

- **Super Carrier Size (`scs`):**
  ```
  +MMscs 1024
  ```
  _(Creates a 1GB super carrier.)_

- **Use Large Pages (`lp`):**
  ```
  +MMlp on
  ```
  _(Enable large/huge pages support.)_

- **Super Carrier Only (`sco`):**
  ```
  +MMsco true
  ```
  _(Allocations occur only within the super carrier.)_

- **Maximum Cached Segments (`mcs`):**
  ```
  +MMmcs 20
  ```
  _(Stores up to 20 cached segments.)_

=== Special Flags for `sys_alloc`

`sys_alloc` interfaces with the system’s malloc:

- **Trim Threshold (`tt`):**  
  Releases memory back to the OS when the free heap exceeds threshold.
  ```
  +MYtt 256
  ```
  _(Set trim threshold to 256 KB.)_

- **Top Pad (`tp`):**  
  Extra memory malloc requests from OS to reduce subsequent calls.
  ```
  +MYtp 512
  ```
  _(malloc requests 512 KB padding.)_

=== Literal Allocator (`literal_alloc`)

Stores literals (compile-time constants):

- **Literal Super Carrier Size (`scs`):**
  ```
  +MIscs 2048
  ```
  _(Set literal allocator super carrier to 2 GB.)_

=== Global and Convenience Flags

- **Minimal/Maximal Allocation Setup (`ea`):**
  ```
  +Mea min|max|config
  ```
  _(Quick configuration presets for all allocators.)_

- **Lock Physical Memory (`lpm`):**
  ```
  +Mlpm all|no
  ```
  _(Locks VM memory into physical RAM.)_

- **Dirty Allocator Instances (`dai`):**
  ```
  +Mdai max|<number>
  ```
  _(Allocator instances specifically for dirty schedulers.)_

---

=== Practical Examples

**Reduce fragmentation with address-order best fit across all allocators:**  
```
+Muas aobf
```

**Limit maximum memory for ETS allocator (e.g., 2 GB):**  
```
+MEamax 2097152
```

**Enable allocation tagging for debugging across all allocators:**  
```
+Muatags true
```

---

=== Recommendations

- Start with default settings unless issues arise.
- Monitor allocator usage (`erlang:system_info/1`, `recon_alloc`) before tuning.
- Incrementally test changes in controlled environments.
- Avoid aggressive tuning without benchmarks and profiling.

[NOTE]
====
Most memory allocator flags described above are highly implementation-dependent.
Their behavior, availability, and defaults can change or be removed entirely
without prior notice. Moreover, the runtime (`erts_alloc`) may ignore or adjust
provided settings based on internal heuristics or system constraints. Always
validate settings with actual system metrics and testing.
====

=== Process Memory

As we saw in xref:CH-Processes[] a process is really just a number
of memory areas, in this chapter we will look a bit closer at how
the stack, the heap and the mailbox are managed.

The default size of the stack and heap is 233 words. This default
size can be changed globally when starting Erlang through the
`pass:[+h]` flag. You can also set the minimum heap size when starting
a process with `spawn_opt` by setting `min_heap_size`.

Erlang terms are tagged as we saw in xref:CH-TypeSystem[], and when
they are stored on the heap they are either cons cells or boxed
objects.


==== Term sharing

Objects on the heap are passed by references within the context of one
process. If you call one function with a tuple as an argument, then
only a tagged reference to that tuple is passed to the called
function. When you build new terms you will also only use references
to sub terms.

For example if you have the string `"hello"` (which is the same as the
list of integers `[104,101,108,108,111]`) you would get a memory layout
similar to:


[[fig-list_layout]]
[ditaa]
----
        ADDR                               BINARY  VALUE + TAG
 hend ->     +-------- -------- -------- --------+
             |                                   |
             |              ...                  |
             |                                   |
             |00000000 00000000 00000000 10000001| 128 + list tag  ----------------+
             |                                   |                                 |
 stop ->     |              ...                  |                                 |
                                                                                   |
                                                                                   |
 htop ->     |              ...                  |                                 |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
         124 |00000000 00000000 00000000 01110001| 112 + list tag  ------------------ | -+
             |                                   |                                    |  |
         120 |00000000 00000000 00000110 01011111| 'e' 101 bsl 4 + small int tag <----+  |
             |                                   |                                       |
         116 |00000000 00000000 00000000 01101001| 104 + list tag  --------------------- | -+
             |                                   |                                       |  |
         112 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <-------+  |
             |                                   |                                          |
         108 |00000000 00000000 00000000 01100001|  96 + list tag  ------------------------ | -+
             |                                   |                                          |  |
         104 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <----------+  |
             |                                   |                                             |
         100 |11111111 11111111 11111111 11111011| NIL                                         |
             |                                   |                                             |
          96 |00000000 00000000 00000110 11111111| 'o' 111 bsl 4 + small int tag <-------------+
             |                                   |
             |                ...                |
             |                                   |
 heap ->     +-----------------------------------+

----

If you then create a tuple with two instances of the list, all that is repeated is
the tagged pointer to the list: `00000000000000000000000010000001`. The code

[source,erlang]
----
L = [104, 101, 108, 108, 111],
T = {L, L}.
----

would result in a memory layout as seen below, with T
pointing to a boxed object at address 136, where we find
an ARITYVAL header saying that this is a tuple of size 2 followed by
its two elements, both pointing to the same list L at address 128.

[ditaa]
----
        ADDR                               BINARY  VALUE + TAG
             |              ...                  |
             |                                   |
             |00000000 00000000 00000000 10001010| 136 + boxed tag  ---+
             |                                   |                     |
 stop ->     |              ...                  |                     |
                                                                       |
                                                                       |
 htop ->     |              ...                  |                     |
             |                                   |                     |
         144 |00000000 00000000 00000000 10000001| 128 + list tag  --- | ----------+
             |                                   |                     |           |
         140 |00000000 00000000 00000000 10000001| 128 + list tag  --- | ----------+
             |                                   |                     |           |
         136 |00000000 00000000 00000000 10000000| 2 + ARITYVAL     <--+           |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
             |              ...                  |                                    :

----

This is nice, since it is cheap to do and uses very little space. But if
you send the tuple to another process or do any other type of IO, or any
operations which results in something called a _deep copy_, then the
data structure is expanded. So if we send the tuple `T` to another process
P2 (`pass:[P2 ! T]`) then the heap of T2 will get a tuple where the first
element points to one copy of the string and the second element to another
copy, doubling the amount of space used. You can see the result of this
in the xref:copied_message[section on message passing] further below.

If you have nested shared tuples, this duplication upon deep copying will
grow exponentially with the level of nesting.
You can quickly bring down your Erlang node by expanding a highly shared term,
see share.erl in the online appendix for the full code example.

[source,erlang]
----
-module(share).

-export([share/2, size/0]).

share(0, Y) -> {Y,Y};
share(N, Y) -> [share(N-1, [N|Y]) || _ <- Y].

size() ->
    T = share:share(5,[a,b,c]),
    {{size, erts_debug:size(T)},
     {flat_size, erts_debug:flat_size(T)}}.



 1> timer:tc(fun() -> share:share(10,[a,b,c]), ok end).
 {1131,ok}

 2> share:share(10,[a,b,c]), ok.
 ok

 3> byte_size(list_to_binary(test:share(10,[a,b,c]))), ok.
 HUGE size (13695500364)
 Abort trap: 6

----

You can calculate the memory size of a shared term and the size of the
expanded size of the term with the functions `erts_debug:size/1` and
`erts_debug:flat_size/1`.

[source,erlang]
----
> share:size().
{{size,19386},{flat_size,94110}}

----

For most applications this is not a problem, but you should be aware
of the problem, which can come up in many situations. A deep copy is
used for IO, ETS tables, binary_to_term, and message passing.

****
It is possible to build ERTS with the configuration option
`--enable-sharing-preserving` which makes the VM discover and preserve
shared terms in these situations, but it is not enabled by default because
it makes sending messages slightly slower in the normal case when there is
no sharing. It has been suggested that this should be the default mode,
since it prevents some very bad situations even if they do not happen
regularly. See xref:AP-BuildingERTS[] for how to build Erlang from source.
****

Let us look in more detail how message passing works.

==== Message passing

When a process P1 sends a message M to another (local) process P2, the
process P1 first calculates the flat size of M. Then it allocates a
new message buffer of that size by doing a heap_alloc of a heap_frag in
the local scheduler context.

Given the code in send.erl (see the online appendix) the state of the system could
look like this just before the send in p1/1:


[ditaa]
----
         REG
             |                                   |
         x0  |00000000 00000000 00000000 00100011| Pid 2
             |                                   |
         x1  |00000000 00000000 00000000 10001010| 136 + boxed tag  ------+
             |                                   |                        |
                                                                          |
                                                                          |
        ADDR                               BINARY  VALUE + TAG            |
 htop ->     |              ...                  |                        |
             |                                   |                        |
         144 |00000000 00000000 00000000 10000001| 128 + list tag  ------ | -------+
             |                                   |                        |        |
         140 |00000000 00000000 00000000 10000001| 128 + list tag  ------ | -------+
             |                                   |                        |        |
         136 |00000000 00000000 00000000 10000000| 2 + ARITYVAL      <----+        |
             |                                   |                                 |
         132 |00000000 00000000 00000000 01111001| 120 + list tag  --------------- | -+
             |                                   |                                 |  |
         128 |00000000 00000000 00000110 10001111| 'h' 104 bsl 4 + small int tag <-+  |
             |                                   |                                    |
         124 |00000000 00000000 00000000 01110001| 112 + list tag  ------------------ | -+
             |                                   |                                    |  |
         120 |00000000 00000000 00000110 01011111| 'e' 101 bsl 4 + small int tag <----+  |
             |                                   |                                       |
         116 |00000000 00000000 00000000 01101001| 104 + list tag  --------------------- | -+
             |                                   |                                       |  |
         112 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <-------+  |
             |                                   |                                          |
         108 |00000000 00000000 00000000 01100001|  96 + list tag  ------------------------ | -+
             |                                   |                                          |  |
         104 |00000000 00000000 00000110 11001111| 'l' 108 bsl 4 + small int tag <----------+  |
             |                                   |                                             |
         100 |11111111 11111111 11111111 11111011| NIL                                         |
             |                                   |                                             |
          96 |00000000 00000000 00000110 11111111| 'o' 111 bsl 4 + small int tag <-------------+
             |                                   |
             |                ...                |

----

Then P1 starts sending the message M to P2. The code in
`erl_message.c` first calculates the flat size of M (which in our example is
23 words)footnote:[We ignore tracing here which will add a trace token
to the size of the message, and always use a heap fragment.].
Then (in a SMP system) if it can take a lock on P2 and there is enough
room on the heap of P2 it will copy the message to the heap of P2.

If P2 is running (or exiting) or there isn't enough space on the heap,
then a new heap fragment is allocated
(of sizeof ErlHeapFragment - sizeof(Eterm) + 23*sizeof(Eterm))
footnote:[The -sizeof(Eterm) comes from mem in ErlHeapFragment already
having the size of 1 Eterm] which after initialization will look like:

----
erl_heap_fragment:
    ErlHeapFragment* next;	    NULL
    ErlOffHeap off_heap:
      erl_off_heap_header* first;   NULL
      Uint64 overhead;                 0
    unsigned alloc_size;	      23
    unsigned used_size;               23
    Eterm mem[1];		       ?
      ... 22 free words
----

Then the message is copied into the `mem` part of the heap fragment, and
the `first` pointer is updated (note that memory addresses increase
downwards in this picture, to match the struct layout):

[[copied_message]]
[ditaa]
----
erl_heap_fragment:
           +--------------------+
           |                    |
           |       ...          |
           |                    |
 first ->  |         mem + BOXED| ----+
           |       ...          |     |
           |                    |     |
   mem ->  |          2+ARITYVAL|  <--+
           |                    |
      +1w  |     3w + mem + CONS|  ---+
           |                    |     |
      +2w  |    13w + mem + CONS| --- | --+
           |                    |     |   |
      +3w  |'H' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +4w  |     5w + mem + CONS|  ---+   |
           |                    |     |   |
      +5w  |'e' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +6w  |     7w + mem + CONS|  ---+   |
           |                    |     |   |
      +7w  |'l' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
      +8w  |     9w + mem + CONS|  ---+   |
           |                    |     |   |
      +9w  |'l' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
     +10w  |    11w + mem + CONS|  ---+   |
           |                    |     |   |
     +11w  |'o' bsl 4 + SMALLINT|  <--+   |
           |                    |         |
     +12w  |                 NIL|         |
           |                    |         |
     +13w  |'H' bsl 4 + SMALLINT|  <------+
           |                    |
     +14w  |    15w + mem + CONS|  ---+
           |                    |     |
     +15w  |'e' bsl 4 + SMALLINT|  <--+
           |                    |
     +16w  |    17w + mem + CONS|  ---+
           |                    |     |
     +17w  |'l' bsl 4 + SMALLINT|  <--+
           |                    |
     +18w  |    19w + mem + CONS|  ---+
           |                    |     |
     +19w  |'l' bsl 4 + SMALLINT|  <--+
           |                    |
     +20w  |    21w + mem + CONS|  ---+
           |                    |     |
     +21w  |'o' bsl 4 + SMALLINT|  <--+
           |                    |
     +22w  |                 NIL|
           |                    |
           +--------------------+

----

In either case a new mbox (`ErlMessage`) is allocated, a lock
 (`ERTS_PROC_LOCK_MSGQ`) is taken on the receiver and the message
 on the heap or in the new heap fragment is linked into the mbox.

[source,c]
----
 erl_mesg {
    struct erl_mesg* next = NULL;
    data:  ErlHeapFragment *heap_frag = bp;
    Eterm m[0]            = message;
 } ErlMessage;

----

Then the mbox is linked into the in message queue (`msg_inq`) of the
receiver, and the lock is released. Note that `msg_inq.last` points to
the `next` field of the last message in the queue. When a new mbox is
linked in this next pointer is updated to point to the new mbox, and
the last pointer is updated to point to the next field of the new
mbox.

[[SS-Binaries]]
==== Binaries

As we saw in xref:CH-TypeSystem[] there are four types of binaries
internally. Three of these types, _heap binaries_, _sub binaries_ and
_match contexts_ are stored on the local heap and handled by the
garbage collector and message passing as any other object, copied as
needed.


===== Reference Counting

The fourth type.  large binaries or _refc binaries_ on the other hand
are partially stored outside of the process heap and they are
reference counted.

The payload of a refc binary is stored in memory allocated by the
binary allocator. There is also a small reference to the payload call
a ProcBin which is stored on the process heap. This reference is
copied by message passing and by the GC, but the payload is
untouched. This makes it relatively cheap to send large binaries to
other processes since the whole binary doesn't need to be copied.

All references through a ProcBin to a refc binary increases the
reference count of the binary by one. All ProcBin objects on a
process heap are linked together in a linked list. After a
GC pass this linked list is traversed and the reference count
of the binary is decreased with one for each ProcBin that
has deceased. If the reference count of the refc binary
reaches zero that binary is deallocated.

Having large binaries reference counted and not copied by send or
garbage collection is a big win, but there is one problem
with having a mixed environment of garbage collection and
reference counting. In a pure reference counted implementation
the reference count would be reduced as soon as a reference to
the object dies, and when the reference count reaches zero the
object is freed. In the ERTS mixed environment a reference to a
reference counted object does not die until a garbage collection
detects that the reference is dead.

This means that binaries, which has a tendency to be large or even
huge, can hang around for a long time after all references to the
binary are dead. Note that since binaries are allocated globally,
all references from all processes need to be dead, that is all
processes that has seen a binary need to do a GC.

Unfortunately it is not always easy, as a developer, to see which
processes have seen a binary in the GC sense of the word seen. Imagine
for example that you have a load balancer that receives work items
and dispatches them to workers.

In <<load_balancer,this code>> there is an example of a loop which
doesn't need to do GC. (See the online appendix for a full example.)

[[load_balancer]]
----
loop(Workers, N) ->
  receive
    WorkItem ->
       Worker = lists:nth(N+1, Workers),
       Worker ! WorkItem,
       loop(Workers, (N+1) rem length(Workers)) 
  end.
----

This server will just keep on grabbing references to binaries and
never free them, eventually using up all system memory.

When one is aware of the problem it is easy to fix, one can either do
a garbage_collect on each iteration of _loop_ or one could do it every
five seconds or so by adding an after clause to the receive. (_after
5000 -> garbage_collect(), loop(Workers, N)_ ).

===== Sub Binaries and Matching

When you match out a part of a binary you get a sub binary.
This sub binary will be a small structure just containing
pointers into the real binary. This increases the reference
count for the binary but uses very little extra space.

If a match would create a new copy of the matched part of the binary
it would cost both space and time. So in most cases just doing a
pattern match on a binary and getting a sub binary to work on is just
what you want.

There are some degenerate cases, imagine for example that you load
huge file like a book into memory and then you match out a small part
like a chapter to work on. The problem is then that the whole of the
rest of the book is still kept in memory until you are done with
processing the chapter. If you do this for many books, perhaps you
want to get the introduction of every book in your file system, then
you will keep the whole of each book in memory and not just the
introductory chapter. This might lead to huge memory usage.

The solution in this case, when you know you only want one small
part of a large binary and you want to have the small part hanging
around for some time, is to use `binary:copy/1`. This function
is only used for its side effect, which is to actually copy
the sub binary out of the real binary removing the reference to
the larger binary and therefore hopefully letting it be garbage
collected.

There is a pretty thorough explanation of how binary construction
and matching is done in the Erlang documentation:
link:http://www.erlang.org/doc/efficiency_guide/binaryhandling.html[].


=== Other interesting memory areas

==== The atom table.

Atoms in Erlang are unique identifiers represented as integers internally. All atoms are stored in a global structure known as the **atom table**. The atom table is a fixed-size structure, meaning there’s an upper limit to how many atoms can exist in a running Erlang system (by default, 1,048,576 atoms). While this might sound like a large number, careless usage (especially dynamically creating atoms from external data) can lead to atom exhaustion, which in turn crashes the entire BEAM VM—an event that's roughly as pleasant as unexpectedly stepping on a LEGO brick in the middle of the night.

Each entry in the atom table contains metadata about an atom, including its string representation (the text of the atom itself), a unique internal identifier used by the runtime system, and additional information like reference counts and details about its usage in modules or functions.

Erlang maintains atoms through three key memory allocator types:

* `atom_text`: Contains the string representations of atoms. This area stores the actual text of atoms.
* `atom_tab`: Stores the atom table itself—a hash table structure for fast lookup.
* `atom_entry`: Allocates memory for each atom's metadata (internal representation, usage counts, etc.).

Atoms are never garbage collected. Once an atom is created, it persists until the VM shuts down. This design decision simplifies implementation (and improves lookup performance), but comes with a risk: an uncontrolled atom creation (commonly through dynamic atom generation via something like `list_to_atom/1`) can lead to exhausting the atom table. Once the atom limit is reached, attempting to create a new atom results in a runtime error, potentially bringing the node down.

Problems often arise when atoms are created carelessly or dynamically, such as when converting user-provided data directly into atoms, parsing large quantities of untrusted external input, or repeatedly generating atoms within loops or recursive functions. These scenarios can lead to rapid atom table growth, potentially exhausting the atom limit and causing severe system issues.

To avoid this:

* Always validate or whitelist user input before converting to atoms.
* Use existing atoms wherever possible or, if dynamic identifiers are required, prefer binaries or strings.
* Monitor the atom table usage regularly using tools like the Observer or built-in functions like 
    - Atom Count (`erlang:system_info(atom_count)`): Number of unique atoms currently loaded.
    - Atom Memory (`erlang:memory(atom)`): Total bytes used by atoms including overhead. 
    - Atom Used Memory (`erlang:memory(atom_used)`): Only the bytes used by the actual atom strings.

A simple check from the Erlang shell can give you a quick indication:

```erlang
1> erlang:system_info(atom_count).
34319
2> erlang:system_info(atom_limit).
1048576
3> erlang:memory(atom).
336049
4> erlang:memory(atom_used).
324520
```

If you ever reach the atom limit, you have two practical solutions:

**Increase the atom table size** (though this is generally a short-term band-aid and should not replace good atom hygiene):

```shell
    erl +t <new_max_atoms>
```

**Redesign your application** to avoid the unlimited creation of atoms—typically by using binaries, strings, or integer identifiers instead.

Thus, atoms and their management require care—misuse can cause stability problems—but when handled correctly, they remain an extremely efficient way of referencing static, known keys or identifiers throughout your system.

To safely convert strings to atoms without risk of atom exhaustion, Erlang provides the `list_to_existing_atom/1` function. This function will only succeed if the atom already exists. If you attempt to create a new atom with this function, it will throw an exception:

[source,erlang]
------------------------------------------

1> list_to_existing_atom("Hello").
** exception error: bad argument
in function  list_to_existing_atom/1
called as list_to_existing_atom("Hello")
*** argument 1: not an already existing atom

2> list_to_existing_atom("true").
true
------------------------------------------


==== Code
Another significant memory area is the code area, where compiled Erlang modules are loaded. Erlang modules, once compiled, are loaded into this code area of memory, which is shared among all processes running within the Erlang runtime system. The code area is generally static and persistent, as modules remain loaded unless explicitly unloaded or replaced (through hot-code loading).

When you load or reload modules using functions such as `l(Module)` or `code:load_file(Module)`, the old code is not immediately removed but kept as the "old" version until no processes reference it. Erlang maintains two versions of each module simultaneously. This allows for safe upgrades without disrupting running processes.

Constants defined in the Erlang code, such as numbers, atoms, and binaries, are stored in a constant pool within the module’s code segment. These constants are efficient in terms of memory usage within a single module, as they are stored only once. However, when constants are used outside of their module—such as during message passing or insertion into ETS tables—they are copied onto the receiving process's heap, potentially increasing overall memory usage significantly.

Monitoring and managing code memory usage is essential, particularly in long-running systems that frequently perform hot-code upgrades. You can inspect loaded modules and their statuses using built-in functions such as `code:all_loaded/0`, and `erlang:memory(code)` to monitor the total memory usage by loaded modules and their constants. 


[source,erlang]
------------------------------------------
1> erlang:memory(code).
6378630
------------------------------------------

